{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical perspective on inverse problems\n",
    "\n",
    "## Formulating prior assumptions\n",
    "\n",
    "We take the viewpoint that what we are measuring, $f^{\\delta}$, is a stochastic variable with mean $f$, which in turn is related to $u$ via the forward operator\n",
    "\n",
    "$$\n",
    "f = Ku.\n",
    "$$\n",
    "\n",
    "Our prior assumptions in this case consist probability distributions for $f^{\\delta}$ and $u$,\n",
    "\n",
    "$$\n",
    "f^\\delta \\sim \\pi_{\\text{data}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "u \\sim \\pi_{\\text{prior}}.\n",
    "$$\n",
    "\n",
    "Using Bayes' rule, we can now formulate the posterior probability density function as\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{post}}(u | f^\\delta) \\propto \\pi_{\\text{data}}(f^\\delta) \\pi_{\\text{prior}}(u),\n",
    "$$\n",
    "\n",
    "where we have ignored the normalizing constants.\n",
    "\n",
    "In some sense, $\\pi_{\\text{post}}(u | f^\\delta)$ is the answer to our inverse problem. It gives us information on the likelihood of any particular $u$ *under the assumptions* we made on $f^\\delta$ and $u$. In some cases, we may be able to express the mean and variance of the resulting posterior density and use those.\n",
    "\n",
    "In all but the simple linear-forward-operator-Gaussian-assumption case, we cannot easily characterize the posterior PDF. We may, however, attempt estimate certain properties by drawing samples from the posterior distribution. Such samples can be generated for any distribution using the Metropolis-Hastings algorithm. This is not very attractive for high-dimensional problems, however. Further discussion of this algorithm is outside the scope of this lecture.\n",
    "\n",
    "### MAP estimation\n",
    "\n",
    "For high-dimensional problems it is common to instead find the most likely parameters\n",
    "\n",
    "$$\n",
    "\\max_{u} \\pi_{\\text{post}}(u|f).\n",
    "$$\n",
    "\n",
    "The $u$ that attains this maximum is called the \\emph{maximum a posteriori} (MAP) estimate. Finding the MAP estimate can be naturally cast as a minimization problem\n",
    "\n",
    "$$\n",
    "\\min_u -\\log \\pi_{\\text{post}}(u|f).\n",
    "$$\n",
    "\n",
    "Analyzing and solving such variational problems will be the subject of subsequent chapters.\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "Let's consider a few examples:\n",
    "\n",
    "### Gaussian\n",
    "With additive Gaussian noise with zero mean and variance $\\sigma$, we express the measurements as\n",
    "\n",
    "$$\n",
    "f^\\delta = Ku + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is normally with zero mean and variance $\\sigma$. Assuming that the $u_i$ are normally distributed with zero mean and unit variance we get\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{post}}(u | f^{\\delta}) = \\exp\\left(-\\frac{1}{2\\sigma^2}\\|Ku - f^{\\delta}\\|_2^2 - \\frac{1}{2}\\|u\\|_2^2\\right),\n",
    "$$\n",
    "\n",
    "which we can re-write as\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{post}}(u | f^{\\delta}) = \\exp\\left(-\\frac{1}{2}\\left({u}^*(\\sigma^{-2}{K}^*K + I)u + \\ldots\\right)\\right),\n",
    "$$\n",
    "\n",
    "so $\\pi_{\\text{post}}$ describes a normal distribution with mean\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "and variance\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{post}} = (\\sigma^{-2}{K}^*K + I)^{-1}.\n",
    "$$\n",
    "\n",
    "It is not hard to show that this coincides with the solution of the Tikhonov least-squares solution with $\\alpha = \\sigma^2$. Indeed, the MAP estimate is obtained by solving\n",
    "\n",
    "$$\n",
    "\\min_{u} \\|{K}u - f\\|_2^2 + \\|u\\|_2^2.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Laplace + uniform\n",
    "\n",
    "If we assume Laplace noise with mean $\\mu$ and unit variance, and a uniform prior $u_i\\in[a_i,b_i]$ we end up with\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{post}}(u | f) = \\exp\\left(-\\|{K}u - f^{\\delta} - \\mu\\|_1\\right)\\prod_i I_{[0,1]}\\left(\\frac{u_i-a_i}{b_i-a_i}\\right)\n",
    "$$\n",
    "\n",
    "The corresponding MAP estimatiob problem is given by\n",
    "\n",
    "$$\n",
    "\\min_{u\\in B} \\|{K}u - f - \\mu\\|_1,\n",
    "$$\n",
    "\n",
    "where $B = \\{u \\in \\mathbb{R}^n \\,|\\, u_i \\in [a_i,b_i]\\,\\, \\text{for}\\,\\, i = 1,2,\\ldots,n\\}$.\n",
    "\n",
    "#### Improper prior\n",
    "In some cases it may not be natural to define prior information in terms of a probability density. For example, the prior information that $u_i \\geq 0$ (all positive values are equally likely) does not have a corresponding probability density function associated with with. We may still add this prior in the Bayesian framework as\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{prior}}(u) = \\prod_iI_{[0,\\infty)}(u_i),\n",
    "$$\n",
    "\n",
    "where $I_{[0,\\infty)}$ is the indicator function which is $1$ with $u_i \\in [0,\\infty)$ and $0$ otherwise.\n",
    "\n",
    "The corresponding variational problem is\n",
    "\n",
    "$$\n",
    "\\min_{u\\geq 0} \\mathcal{J}(u,f^\\delta),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{J}(u,f^\\delta) = -\\log\\pi_{\\text{post}}(u | f^\\delta)$.\n",
    "\n",
    "\n",
    "#### Poisson noise\n",
    "We have seen that Poisson noise also plays an important role in many applications. In this case, we cannot model the noise as additive. Instead, we can view the observations $f_i^{\\delta}$ as a stochastic variable having a Poisson distribution with parameter $\\lambda_i = \\left({K}u\\right)_i$. This leads to\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{data}}(u|f^{\\delta}) = \\prod_i \\frac{ \\left({K}u\\right)_i^{f_i^\\delta} }{f_i^\\delta!}\n",
    "\\exp\\left({-\\left({K}u\\right)_i}\\right).\n",
    "$$\n",
    "\n",
    "The corresponding variational problem is \n",
    "\n",
    "$$\n",
    "\\min_{u} \\sum_{i=1}^m \\left(\\left(\\mathcal{K}u\\right)_i - f_i^{\\delta}\\ln\\left(\\mathcal{K}u\\right)_i\\right).\n",
    "$$\n",
    "\n",
    "#### Gaussian random fields\n",
    "\n",
    "To include spatial correlations we can model $u$ as being normally distributed with mean $\\mu$ and \\emph{covariance} $\\Sigma_{\\text{prior}}$. Popular choices are\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{prior},ij} = \\exp\\left(-\\frac{|i-j|^p}{pL^{p}}\\right),\n",
    "$$\n",
    "\n",
    "where $L$ denotes the correlation length and $p$ is a parameter.\n",
    "\n",
    "The corresponding variational problem is\n",
    "\n",
    "$$\n",
    "\\min_u \\mathcal{J}(u,f^\\delta) + \\|u\\|_{\\Sigma^{-1}_{\\text{prior}}}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
