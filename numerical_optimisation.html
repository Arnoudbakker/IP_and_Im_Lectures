

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>6. Numerical optimisation for inverse problems &#8212; 10 Lectures on Inverse Problems and Imaging</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/mystnb.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Image processing" href="image_processing.html" />
    <link rel="prev" title="5. Variational formulations for inverse problems" href="variational_formulations.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">10 Lectures on Inverse Problems and Imaging</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="what_is.html">
   1. What is an inverse problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_ip_regularization.html">
   2. Discrete Inverse Problems and Regularisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ip_function_spaces.html">
   3. Linear inverse problems in function spaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistical_perspective.html">
   4. A statistical perspective on inverse problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="variational_formulations.html">
   5. Variational formulations for inverse problems
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Numerical optimisation for inverse problems
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="image_processing.html">
   1. Image processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tomography.html">
   2. Computed Tomography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wavefield_imaging.html">
   3. Wavefield Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="magnetic_resonance_imaging.html">
   4. Magnetic Resonance Imaging
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preliminaries
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   1. Linear algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="functional_analysis.html">
   2. Functional analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fourier_sampling.html">
   3. Fourier transform, distributions and sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistics.html">
   4. Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="variational_analysis.html">
   5. Variational analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convex_analysis.html">
   6. Convex analysis
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/numerical_optimisation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/TristanvanLeeuwen/IP_and_Im_Lectures/edit/master/numerical_optimisation.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#smooth-optimisation">
   6.1. Smooth optimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     6.1.1. Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linesearch">
     6.1.2. Linesearch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#second-order-methods">
     6.1.3. Second order methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-optimisation">
   6.2. Convex optimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     6.2.1. Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proximal-gradient-methods">
     6.2.2. Proximal gradient methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#splitting-methods">
     6.2.3. Splitting methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   6.3. References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   6.4. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#steepest-descent-for-strongly-convex-functionals">
     6.4.1. Steepest descent for strongly convex functionals
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rosenbrock">
     6.4.2. Rosenbrock
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convex-conjugates">
     6.4.3. Convex conjugates
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subdifferentials">
     6.4.4. Subdifferentials
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-tresholding">
     6.4.5. Soft tresholding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-dual-method-for-tv-denoising">
     6.4.6. A dual method for TV denoising
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-prima-dual-method-for-tv-denoising">
     6.4.7. A Prima-dual method for TV denoising
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assignments">
   6.5. Assignments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spline-regularisation">
     6.5.1. Spline regularisation
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="numerical-optimisation-for-inverse-problems">
<h1><span class="section-number">6. </span>Numerical optimisation for inverse problems<a class="headerlink" href="#numerical-optimisation-for-inverse-problems" title="Permalink to this headline">¶</a></h1>
<p>In this chapter we treat numerical algorithms for solving optimisation problems over <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Throughout we will assume that the objective <span class="math notranslate nohighlight">\(J(u) = D(u) + R(u)\)</span> satisfies the conditions for a minimiser to exist. We distinguish between two important classes of problems; <em>smooth</em> problems and <em>convex</em> problems.</p>
<div class="section" id="smooth-optimisation">
<h2><span class="section-number">6.1. </span>Smooth optimisation<a class="headerlink" href="#smooth-optimisation" title="Permalink to this headline">¶</a></h2>
<p>For smooth problems, we have assume to have access to as many derivatives of <span class="math notranslate nohighlight">\(J\)</span> as we need. As before, we denote the first derivative (or gradient) by <span class="math notranslate nohighlight">\(J' : \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span>. We denote the second derivative (or Hessian) by <span class="math notranslate nohighlight">\(J'' : \mathbb{R}^n \rightarrow \mathbb{R}^{n\times n}\)</span>. We will additionally assume that the Hessian is globally bounded, i.e. there exists a constant <span class="math notranslate nohighlight">\(L &lt; \infty\)</span> such that <span class="math notranslate nohighlight">\(J''(u) \preceq L\cdot I\)</span> for all <span class="math notranslate nohighlight">\(u\in\mathbb{R}^n\)</span>. Note that this implies that <span class="math notranslate nohighlight">\(J'\)</span> is Lipschitz continous with constant <span class="math notranslate nohighlight">\(L\)</span>: <span class="math notranslate nohighlight">\(\|J'(u) - J'(v)\|_2 \leq L \|u - v\|_2\)</span>.</p>
<p>For a comprehensive treatment of this topic (and many more), we recommend the seminal book <em>Numerical Optimization</em> by Stephen Wright and Jorge Nocedal <a class="bibtex reference internal" href="what_is.html#nocedal2006numerical" id="id1">[2]</a>.</p>
<hr class="docutils" />
<p>Before discussing optimisation methods, we first introduce the optimality conditions.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Optimality conditions</em></p>
<p>Given a smooth functional <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\rightarrow \mathbb{R}\)</span>, a point <span class="math notranslate nohighlight">\(u_* \in \mathbb{R}^n\)</span> is local minimiser iff it satisfies the first and second order optimality conditions</p>
<div class="math notranslate nohighlight">
\[J'(u_*) = 0, \quad J''(u_*) \succeq 0.\]</div>
<p>If <span class="math notranslate nohighlight">\(J''(u_*) \succ 0\)</span> we call <span class="math notranslate nohighlight">\(u_*\)</span> a <em>strict</em> local minimiser.</p>
</div>
<div class="section" id="gradient-descent">
<h3><span class="section-number">6.1.1. </span>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>The steepest descent method proceeds to find a minimiser through a fixed-point iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I - \lambda J'\right)(u_k) = u_k - \lambda J'(u_k),\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> is the stepsize. The following theorem states that this iteration will yield a fixed point of <span class="math notranslate nohighlight">\(J\)</span>, regardless of the initial iterate, provided that we pick <span class="math notranslate nohighlight">\(\lambda\)</span> small enough.</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Global convergence of steepest descent</em></p>
<p>Let <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\rightarrow \mathbb{R}\)</span> be a smooth, Lipschitz-continuos functional. The fixed point iteration</p>
<div class="math notranslate nohighlight" id="equation-steepest-descent">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-steepest-descent" title="Permalink to this equation">¶</a></span>\[u_{k+1} = \left(I - \lambda J'\right)(u_k),\]</div>
<p>with <span class="math notranslate nohighlight">\(\lambda \in (0,L/2)\)</span> produces iterates <span class="math notranslate nohighlight">\(u_k\)</span> for which</p>
<div class="math notranslate nohighlight">
\[\min_{k\in \{0,1,\ldots, n\}} \|J'(u_k)\|_2^2 \leq \frac{J(u_0) - J_*}{C (n+1)},\]</div>
<p>with <span class="math notranslate nohighlight">\(C = \lambda \left( 1 - \textstyle{\frac{\lambda L}{2}}\right)\)</span> and <span class="math notranslate nohighlight">\(J_* = \min_u J(u)\)</span>. This implies that <span class="math notranslate nohighlight">\(J'(u_k) \rightarrow 0\)</span> as <span class="math notranslate nohighlight">\(k\rightarrow \infty\)</span> at a <em>sublinear rate</em> of <span class="math notranslate nohighlight">\(\mathcal{O}(1/\sqrt{k})\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<p>Start from a Taylor expansion:</p>
<div class="math notranslate nohighlight">
\[J(u_{k+1}) = J(u_k) + J'(u_k)(u_{k+1} - u_k) + \textstyle{\frac{1}{2}}(u_{k+1} - u_k)^T J''(\eta_k)(u_{k+1} - u_k).\]</div>
<p>Now bound the last term using the fact that <span class="math notranslate nohighlight">\(J''(u) \succeq L\cdot I\)</span> and plug in <span class="math notranslate nohighlight">\(u_{k+1} - u_k = -\lambda J'(u_k)\)</span> to get</p>
<div class="math notranslate nohighlight">
\[J(u_{k+1}) - J(u_k) \leq \lambda \left( \textstyle{\frac{\lambda L}{2}} - 1\right) \|J'(u_k)\|_2^2.\]</div>
<p>We conclude that for <span class="math notranslate nohighlight">\(0 &lt; \lambda &lt; \textstyle{\frac{1}{2L}}\)</span> we have that <span class="math notranslate nohighlight">\(J(u_{k+1}) &lt; J(u_k)\)</span> unless <span class="math notranslate nohighlight">\(\|J'(u_k)\|_2 = 0\)</span>, in which case <span class="math notranslate nohighlight">\(u_k\)</span> is a stationary point. Now, sum over <span class="math notranslate nohighlight">\(k\)</span> and re-organise to get</p>
<div class="math notranslate nohighlight">
\[\sum_{k=0}^n \|J'(u_k)\|_2^2 \leq \frac{J(u_0) - J(u_n)}{C},\]</div>
<p>with <span class="math notranslate nohighlight">\(C = \lambda \left( 1 - \textstyle{\frac{\lambda L}{2}}\right)\)</span>. Since <span class="math notranslate nohighlight">\(J_* \leq J(u_n)\)</span> we obtain the desired result.</p>
</div>
</div>
<p>A stronger statement on convergence can be made by making additional assumptions on <span class="math notranslate nohighlight">\(J\)</span> (such as convexity), but this is left as an exercise.</p>
</div>
<div class="section" id="linesearch">
<h3><span class="section-number">6.1.2. </span>Linesearch<a class="headerlink" href="#linesearch" title="Permalink to this headline">¶</a></h3>
<p>While the previous results are nice in theory, we usually do not have access to the Lipschitz constant <span class="math notranslate nohighlight">\(L\)</span>. This could lead us to pick a very small stepsize, which would yield a very slow convergence in practice. A popular way of choosing a stepsize adaptively is a <em>linesearch</em> strategy. To introduce these, we slightly broaden the scope and consider the iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = u_k + \lambda_k d_k,\]</div>
<p>where <span class="math notranslate nohighlight">\(d_k\)</span> is a <em>descent direction</em> satisfying <span class="math notranslate nohighlight">\(d_k^TJ'(u_k) &lt; 0\)</span>. Obviously, <span class="math notranslate nohighlight">\(d_k = - J'(u_k)\)</span> is a descent direction, but other choices may be beneficial in practice. In particular, we can choose <span class="math notranslate nohighlight">\(d_k = -B J'(u_k)\)</span> for any positive-definite matrix <span class="math notranslate nohighlight">\(B\)</span> to obtain a descent direction. How to choose such a matrix will be discussed in the next section.</p>
<p>Two important linesearch methods are discussed below.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Backtracking linesearch</em></p>
<p>In order to ensure sufficient progress of the iterations, we can choose a steplength that guarantees sufficient descent:</p>
<div class="math notranslate nohighlight">
\[J(u_k + \lambda d_k) \leq J(u_k) + c_1 \lambda J'(u_k)^Td_k,\]</div>
<p>with <span class="math notranslate nohighlight">\(c_1 \in (0,1)\)</span> a small constant (typically <span class="math notranslate nohighlight">\(c_1 = 10^{-4}\)</span>). Existence of a <span class="math notranslate nohighlight">\(\lambda\)</span> satisfying these conditions is guaranteed by the regularity of <span class="math notranslate nohighlight">\(J\)</span>. We can find a suitable <span class="math notranslate nohighlight">\(\lambda\)</span> by <em>backtracking</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">J</span><span class="p">,</span><span class="n">Jp</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">lmbda</span><span class="p">,</span><span class="n">rho</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">c1</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Backtracking linesearch to find a stepsize satisfying J(u + lmbda*d) &lt;= J(u) + lmbda*c1*J(u)^Td</span>

<span class="sd">Input:</span>
<span class="sd">  J  - Function object returning the value of J at a given input vector</span>
<span class="sd">  Jp - Function object returning the gradient of J at a given input vector</span>
<span class="sd">  u  - current iterate as array of length n</span>
<span class="sd">  d  - descent direction as array of length n</span>
<span class="sd">  lmbda - initial stepsize</span>
<span class="sd">  rho,c1 - backtracking parameters, default (0.5,1e-4)</span>

<span class="sd">Output:</span>
<span class="sd">  lmbda - stepsize satisfying the sufficient decrease condition</span>
<span class="sd">&quot;&quot;&quot;</span>
  <span class="k">while</span> <span class="n">J</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">lmbda</span><span class="o">*</span><span class="n">d</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">J</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span><span class="o">*</span><span class="n">lmbda</span><span class="o">*</span><span class="n">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">lmbda</span> <span class="o">*=</span> <span class="n">rho</span>
  <span class="k">return</span> <span class="n">lmbda</span>
</pre></div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Wolfe linesearch</em></p>
<p>A possible disadvantage of the backtracking linesearch introduced earlier is that it may end up choosing very small stepsizes. To obtain a stepsize that yields a new iterate at which the slope of <span class="math notranslate nohighlight">\(J\)</span> is not too large, we introduce the following condition</p>
<div class="math notranslate nohighlight">
\[|J'(u_k + \lambda d_k)^Td_k| \leq c_2 |J'(u_k)^Td_k|,\]</div>
<p>where <span class="math notranslate nohighlight">\(c_2\)</span> is a small constant satisfying <span class="math notranslate nohighlight">\(0 &lt; c_1 &lt; c_2 &lt; 1\)</span>. Together with the sufficient descent condition, these are referred to as the <em>strong Wolfe conditions</em>. Existence of a stepsize satisfying these conditions is again guaranteed by the regularity of <span class="math notranslate nohighlight">\(J\)</span> (cf. <a class="bibtex reference internal" href="what_is.html#nocedal2006numerical" id="id2">[2]</a>, lemma 3.1). Finding such a <span class="math notranslate nohighlight">\(\lambda\)</span> is a little more involved than the backtracking procedure outlined above (cf. <a class="bibtex reference internal" href="what_is.html#nocedal2006numerical" id="id3">[2]</a>, algorithm 3.5). Luckily, the <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> library provides an implementation of this algorithm (cf. <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html"><code class="docutils literal notranslate"><span class="pre">scipy.optimize.line_search</span></code></a>)</p>
</div>
</div>
<div class="section" id="second-order-methods">
<h3><span class="section-number">6.1.3. </span>Second order methods<a class="headerlink" href="#second-order-methods" title="Permalink to this headline">¶</a></h3>
<p>A well-known method for rootfinding is <em>Newton’s method</em>, which finds a root for which <span class="math notranslate nohighlight">\(J'(u) = 0\)</span> via the fixed point iteration</p>
<div class="math notranslate nohighlight" id="equation-newton">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-newton" title="Permalink to this equation">¶</a></span>\[u_{k+1} = u_k - J''(u_k)^{-1}J(u_k).\]</div>
<p>We can interpret this method as finding the new iterate <span class="math notranslate nohighlight">\(u_{k+1}\)</span> as the (unique) minimiser of the quadratic approximation of <span class="math notranslate nohighlight">\(J\)</span> around <span class="math notranslate nohighlight">\(u_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[J(u) \approx J(u_k) + J'(u_k)(u - u_k) + \textstyle{\frac{1}{2}}(u-u_k)^T J''(u_k)(u-u_k).\]</div>
<div class="admonition-theorem-convergence-of-newton-s-method admonition">
<p class="admonition-title">Theorem: <em>Convergence of Newton’s method</em></p>
<p>Let <span class="math notranslate nohighlight">\(J\)</span> be a smooth functional and <span class="math notranslate nohighlight">\(u_*\)</span> be a (local) minimiser. For any <span class="math notranslate nohighlight">\(u_0\)</span> sufficiently close to <span class="math notranslate nohighlight">\(u_*\)</span>, the iteration <a class="reference internal" href="#equation-newton">(6.2)</a> converges quadratically to <span class="math notranslate nohighlight">\(u_*\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\|u_{k+1} - u_*\|_2 \leq M \|u_k - u_*\|_2^2,\]</div>
<p>with <span class="math notranslate nohighlight">\(M = 2\|J'''(u_*)\|_2 \|J''(u_*)^{-1}\|_2\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<p>See <code class="docutils literal notranslate"><span class="pre">cite</span></code>{nocedal2006numerical}, Thm. 3.5.</p>
</div>
</div>
<hr class="docutils" />
<p>In some applications, it may be difficult to compute and invert the Hessian. This leads to so-called <em>quasi-Newton</em> methods which approximate the Hessian. The basis for such approximations is the <em>secant relation</em></p>
<div class="math notranslate nohighlight">
\[H_k (u_{k+1} - u_k) = (J'(u_{k+1}) - J'(u_k)),\]</div>
<p>which is satisfied by the true Hessian <span class="math notranslate nohighlight">\(J''\)</span> at a point <span class="math notranslate nohighlight">\(\eta_k = u_k + t(u_* - u_k)\)</span> for some <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>. Obviously, we cannot hope to solve for <span class="math notranslate nohighlight">\(H_k \in \mathbb{R}^{n\times n}\)</span> from just these <span class="math notranslate nohighlight">\(n\)</span> equations. We can, however, impose some structural assumptions on the Hessian. Assuming a simple diagonal structure <span class="math notranslate nohighlight">\(H_k = h_k I\)</span> yields <span class="math notranslate nohighlight">\(h_k = (J'(u_{k+1}) - J'(u_k))^T(u_{k+1} - u_k)/\|u_{k+1} - u_k\|_2^2\)</span>. In fact, even gradient-descent can be interpreted in this manner by approximating <span class="math notranslate nohighlight">\(J''(u_k) \approx L I\)</span>.</p>
<hr class="docutils" />
<p>An often-used approximation is the <em>Broyden-Fletcher-Goldfarb-Shannon (BFGS)</em> approximation, which keep track of the steps <span class="math notranslate nohighlight">\(s_k = u_{k+1} - u_k\)</span>
and gradients <span class="math notranslate nohighlight">\(y_k = J'(u_{k+1}) - J'(u_k)\)</span> to recursively construct an approximation of the <em>inverse</em> of the Hessian as</p>
<div class="math notranslate nohighlight">
\[B_{k+1} = \left(I - \rho_k s_k y_k^T\right)H_k\left(I - \rho_k y_k s_k^T\right) + \rho_k s_ks_k^T,\]</div>
<p>with <span class="math notranslate nohighlight">\(\rho_k = (s_k^Ty_k)^{-1}\)</span> and <span class="math notranslate nohighlight">\(B_0\)</span> choses appropriately (e.g., B_0 = L^{-1} \cdot I). It can be shown that this approximation is sufficiently accurate to yield <em>superlinear</em> convergence when using a Wolfe linesearch.</p>
<hr class="docutils" />
<p>The are many practical aspects to implementing such methods. For example, what do we do when the approximated Hessian becomes (near) singular? Discussing these issues is beyond the scope of these lecture notes and we refer to <a class="bibtex reference internal" href="what_is.html#nocedal2006numerical" id="id4">[2]</a> chapter 6 for more details. The <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> library provides an implementation of <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html">various optimisation methods</a>.</p>
</div>
</div>
<div class="section" id="convex-optimisation">
<h2><span class="section-number">6.2. </span>Convex optimisation<a class="headerlink" href="#convex-optimisation" title="Permalink to this headline">¶</a></h2>
<p>In this section, we consider finding a minimiser of a <em>convex</em> functional <span class="math notranslate nohighlight">\(J : \mathbb{R}^n \rightarrow \mathbb{R}_{\infty}\)</span>. Note that we allow the functionals to take values on the extended real line. We accordingly define the domain of <span class="math notranslate nohighlight">\(J\)</span> as <span class="math notranslate nohighlight">\(\text{dom}(J) = \{u \in \mathbb{R}^n \, | \, J(u) &lt; \infty\}\)</span>.</p>
<p>To deal with convex functionals that are not smooth, we first generalise the notion of a derivative.</p>
<div class="important admonition">
<p class="admonition-title">Definition: subgradient</p>
<p>Given a convex functional <span class="math notranslate nohighlight">\(J\)</span>, we call <span class="math notranslate nohighlight">\(g \in \mathbb{R}^n\)</span> a subgradient of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(u\)</span> if</p>
<div class="math notranslate nohighlight">
\[J(v) \geq J(u) + g^T(v - u) \quad \forall v \in \mathbb{R}^n.\]</div>
<p>This definition is reminiscent of the Taylor expansion and we can indeed easily check that it holds for convex smooth functionals for <span class="math notranslate nohighlight">\(g = J'(u)\)</span>. For non-smooth functionals there may be multiple vectors <span class="math notranslate nohighlight">\(g\)</span> satisfying the inequality. We call the set of all such vectors the <em>subdifferential</em> which we will denote as <span class="math notranslate nohighlight">\(J'(u)\)</span>. Note that we deviate from the more usual notation <span class="math notranslate nohighlight">\(\partial J\)</span> to make the transition from the previous section seemless.</p>
</div>
<div class="admonition-example-subdifferentials-of-some-functions admonition">
<p class="admonition-title">Example: Subdifferentials of some functions</p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J_1(u) = |u|\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(J_2(u) = \delta_{[0,1]}(u)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(J_3(u) = \max\{u,0\}\)</span>.</p></li>
</ul>
<p>All these functions are convex and exhibit a discontinuity in the derivative at <span class="math notranslate nohighlight">\(u = 0\)</span>. The subdifferentials at <span class="math notranslate nohighlight">\(u=0\)</span> are given by</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J_1'(u) = [-1,1]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J_2'(u) = (-\infty,0]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J_3'(u) = [0,1]\)</span></p></li>
</ul>
<div class="figure align-default" id="convex-examples" style="width: 600px">
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_1_0.png" src="_images/numerical_optimisation_1_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Examples of several convex functions and an element of their subdifferential at <span class="math notranslate nohighlight">\(u=0\)</span>.</span><a class="headerlink" href="#convex-examples" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>

<span class="c1">#</span>
<span class="n">J1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
<span class="n">J2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="n">u</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">],[</span><span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mf">1e6</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mf">1e6</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">J3</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="n">u</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">],[</span><span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">u</span><span class="p">])</span>

<span class="c1">#</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">J1</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">.</span><span class="mi">1</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">J2</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">J3</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">.</span><span class="mi">9</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;convex_examples&quot;</span><span class="p">,</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_1_0.png" src="_images/numerical_optimisation_1_0.png" />
<img alt="_images/numerical_optimisation_1_1.png" src="_images/numerical_optimisation_1_1.png" />
</div>
</div>
<p>Some useful calculus rules for subgradients are listed below.</p>
<div class="admonition-theorem-computing-subgradients admonition">
<p class="admonition-title">Theorem: Computing subgradients</p>
<p>Let <span class="math notranslate nohighlight">\(J_i:\mathbb{R}^n \rightarrow \mathbb{R}_{\infty}\)</span> be a proper convex functionals and A\in\mathbb{R}{n\times n}$, <span class="math notranslate nohighlight">\(b \in \mathbb{R}^n\)</span>.</p>
<p>We then have the following usefull rules</p>
<ul class="simple">
<li><p><em>summation:</em> <span class="math notranslate nohighlight">\(J_1'(u) + J_2'(u)\)</span> for <span class="math notranslate nohighlight">\(u\)</span> in the interior of <span class="math notranslate nohighlight">\(\text{dom}(J)\)</span>.</p></li>
<li><p><em>affine transformation:</em> <span class="math notranslate nohighlight">\(\left(J(Au + b)\right)' = A^T J'(Au + b)\)</span> for <span class="math notranslate nohighlight">\(u, Au + b\)</span> in the interior of <span class="math notranslate nohighlight">\(\text{dom}(J)\)</span>.</p></li>
</ul>
<p>An overview of more useful relations can be found in e.g., <a class="bibtex reference internal" href="what_is.html#beck2017" id="id5">[1]</a> section 3.8.</p>
</div>
<hr class="docutils" />
<p>With this we can now formulate optimality conditions for convex optimisation.</p>
<div class="important admonition">
<p class="admonition-title">Definition: Optimality conditions for convex optimisation</p>
<p>Let <span class="math notranslate nohighlight">\(J:\mathbb{R}^n \rightarrow \mathbb{R}_{\infty}\)</span> be a proper convex functional. A point <span class="math notranslate nohighlight">\(u_* \in \mathbb{R}^n\)</span> is a minimiser iff</p>
<div class="math notranslate nohighlight">
\[0 \in J'(u_*).\]</div>
</div>
<div class="admonition-example-computing-the-median admonition">
<p class="admonition-title">Example: <em>Computing the median</em></p>
<p>The median <span class="math notranslate nohighlight">\(u\)</span> of a set of numbers <span class="math notranslate nohighlight">\((f_1, f_2, \ldots, f_n)\)</span> is a minimiser of</p>
<div class="math notranslate nohighlight">
\[J(u) = \sum_{i=1}^n |u - f_i|.\]</div>
<p>Introducing <span class="math notranslate nohighlight">\(J_i = |u - f_i|\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}J_i'(u) = \begin{cases} -1 &amp; u &lt; f_i \\ [-1,1] &amp; u = f_i \\ 1 &amp; u &gt; f_i\end{cases},\end{split}\]</div>
<p>with which we can compute <span class="math notranslate nohighlight">\(J'(u)\)</span> using the sum-rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}J'(u) = \begin{cases} -n &amp; u &lt; f_1 \\ 2i - n &amp; u \in (a_i,a_{i+1})\\ 2i-1-n+[-1,1] &amp; u = f_i\\n &amp; f&gt; f_n\end{cases}.\end{split}\]</div>
<p>To find a <span class="math notranslate nohighlight">\(u\)</span> for which <span class="math notranslate nohighlight">\(0\in J'(u)\)</span> we need to consider the middle two cases. If <span class="math notranslate nohighlight">\(n\)</span> is even, we can find an <span class="math notranslate nohighlight">\(i\)</span> such that <span class="math notranslate nohighlight">\(2i = n\)</span> and get that for all <span class="math notranslate nohighlight">\(u \in [f_{n/2},f_{n/2+1}]\)</span> we have <span class="math notranslate nohighlight">\(0 \in J'(u)\)</span>.
When <span class="math notranslate nohighlight">\(n\)</span> is odd, we have optimality only for <span class="math notranslate nohighlight">\(u = f_{(n+1)/2}\)</span>.</p>
<div class="figure align-default" id="median-example" style="width: 600px">
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_3_0.png" src="_images/numerical_optimisation_3_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Subgradient of <span class="math notranslate nohighlight">\(J\)</span> for <span class="math notranslate nohighlight">\(f=(1,2,3,4)\)</span> and <span class="math notranslate nohighlight">\(f=(1,2,3,4,5)\)</span>.</span><a class="headerlink" href="#median-example" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>

<span class="c1">#</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">f2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>

<span class="c1">#</span>
<span class="n">Jip</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span><span class="n">f</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">u</span><span class="p">,[</span><span class="n">u</span><span class="o">&lt;</span><span class="n">f</span><span class="p">,</span><span class="n">u</span><span class="o">==</span><span class="n">f</span><span class="p">,</span><span class="n">u</span><span class="o">&gt;</span><span class="n">f</span><span class="p">],[</span><span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f</span><span class="p">):</span>
  <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">Jip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">g</span>

<span class="c1">#</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f1</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$f = [1,2,3,4]$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f2</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$f = [1,2,3,4,5]$&#39;</span><span class="p">)</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;median_example&quot;</span><span class="p">,</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_3_0.png" src="_images/numerical_optimisation_3_0.png" />
<img alt="_images/numerical_optimisation_3_1.png" src="_images/numerical_optimisation_3_1.png" />
</div>
</div>
<div class="section" id="id6">
<h3><span class="section-number">6.2.1. </span>Gradient descent<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>A natural extension of the gradient-descent method for smooth problems is the <em>subgradient descent method</em>:</p>
<div class="math notranslate nohighlight" id="equation-subgradient-descent">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-subgradient-descent" title="Permalink to this equation">¶</a></span>\[u_{k+1} = u_k - \lambda g_k, \quad g_k \in J'(u_k).\]</div>
<p>Note that this can be interpreted as a fixed-point iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I - \lambda J'\right)(u_k).\]</div>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Convergene of subgradient descent</em></p>
<p>Let ..</p>
</div>
</div>
<div class="section" id="proximal-gradient-methods">
<h3><span class="section-number">6.2.2. </span>Proximal gradient methods<a class="headerlink" href="#proximal-gradient-methods" title="Permalink to this headline">¶</a></h3>
<p>While the subgradient descent method is easily implemented, it does not fully exploit the structure of the objective. In particular, we can often split the objective in a <em>smooth</em> and a <em>convex</em> part. For the discussion we will assume for the moment that <span class="math notranslate nohighlight">\(J(u) = D(u) + R(u)\)</span> where <span class="math notranslate nohighlight">\(D\)</span> is smooth and <span class="math notranslate nohighlight">\(R\)</span> is convex. We are then looking for a point <span class="math notranslate nohighlight">\(u_*\)</span> for which</p>
<div class="math notranslate nohighlight" id="equation-diff-inclusion">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-diff-inclusion" title="Permalink to this equation">¶</a></span>\[D'(u_*) \in -R'(u_*).\]</div>
<p>Finding such a point can be done (again!) by a fixed-point iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I + \lambda R'\right)^{-1}\left(I - \lambda D'\right)(u_k),\]</div>
<p>where <span class="math notranslate nohighlight">\(u = \left(I + \lambda R'\right)^{-1}(v)\)</span> yields a point <span class="math notranslate nohighlight">\(u\)</span> for which <span class="math notranslate nohighlight">\(\lambda^{-1}(v - u) \in R'(u)\)</span>. We can easily show that a fixed point of this iteration indeed solves the differential inclusion problem <a class="reference internal" href="#equation-diff-inclusion">(6.4)</a>. Assuming a fixed point <span class="math notranslate nohighlight">\(u_*\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[u_{*} = \left(I + \lambda R'\right)^{-1}\left(I - \lambda D'\right)(u_*),\]</div>
<p>using the definition of <span class="math notranslate nohighlight">\(\left(I + \lambda R'\right)^{-1}\)</span> this yields</p>
<div class="math notranslate nohighlight">
\[\lambda^{-1}\left(u_* - \lambda D'(u_*) - u_*\right) \in R'(u_*),\]</div>
<p>which indeed confirms that <span class="math notranslate nohighlight">\(-D'(u_*) \in R'(u_*)\)</span>.</p>
<hr class="docutils" />
<p>The operator <span class="math notranslate nohighlight">\(\left(I + \lambda R'\right)^{-1}\)</span> is called the <em>proximal operator</em> of <span class="math notranslate nohighlight">\(\lambda R\)</span>, whose action on input <span class="math notranslate nohighlight">\(v\)</span> is implicitly defined as solving</p>
<div class="math notranslate nohighlight">
\[\min_u \textstyle{\frac{1}{2}} \|u - v\|_2^2 + \lambda R(u).\]</div>
<p>We usually denote this operator by <span class="math notranslate nohighlight">\(\text{prox}_{\lambda R}(v)\)</span>. With this, the proximal gradient method for solving <a class="reference internal" href="#equation-diff-inclusion">(6.4)</a> is then denoted as</p>
<div class="math notranslate nohighlight" id="equation-proximal-gradient">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-proximal-gradient" title="Permalink to this equation">¶</a></span>\[u_{k+1} = \text{prox}_{\lambda R}\left(u_k - \lambda D'(u_k)\right).\]</div>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Convergence of the proximal point iteration</em></p>
<p>Let <span class="math notranslate nohighlight">\(J = D + R\)</span> be a functional with <span class="math notranslate nohighlight">\(D\)</span> smooth and <span class="math notranslate nohighlight">\(R\)</span> convex. Denote the Lipschitz constant of <span class="math notranslate nohighlight">\(D'\)</span> by <span class="math notranslate nohighlight">\(L_D\)</span>. The iterates produced by <a class="reference internal" href="#equation-proximal-gradient">(6.5)</a> with a fixed stepsize <span class="math notranslate nohighlight">\(\lambda = 1/L_D\)</span> converge to a stationary point <span class="math notranslate nohighlight">\(u_*\)</span> for which $u_* = \text{prox}<em>{\lambda R}\left(u</em>* - \lambda D’(u_*)\right).</p>
<p>If, in addition, <span class="math notranslate nohighlight">\(D\)</span> is convex the iterates converges sublinearly to a minimiser <span class="math notranslate nohighlight">\(u_*\)</span>:</p>
<div class="math notranslate nohighlight">
\[J(u_k) - J_* \leq \frac{L_D \|u_* - u_0\|_2^2}{2k}.\]</div>
<p>If <span class="math notranslate nohighlight">\(D\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>-strongly convex, the iteration converges linearly to a minimiser <span class="math notranslate nohighlight">\(u_*\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|u_{k+1} - u_*\|_2^2 \leq \left(1 - \mu/L_D\right) \|u_{k} - u_*\|_2^2.\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<p>We refer to <a class="bibtex reference internal" href="what_is.html#beck2017" id="id7">[1]</a> Thms. 10.15, 10.21, and 10.29 or more details.</p>
</div>
</div>
<hr class="docutils" />
<p>When compared to the subgradient method, we may expect better performance from the proximal gradient method when <span class="math notranslate nohighlight">\(D\)</span> is strongly convex and <span class="math notranslate nohighlight">\(R\)</span> is convex. Even if <span class="math notranslate nohighlight">\(J\)</span> is smooth, the proximal gradient method may be favourable as the convergence constants depend on the Lipschitz constant of <span class="math notranslate nohighlight">\(D\)</span> only; not <span class="math notranslate nohighlight">\(J\)</span>. All this comes at the cost of solving a minimisation problem at each iteration, so these methods are usually only applied when a closed-form expression for the proximal operator exists.</p>
</div>
<div class="section" id="splitting-methods">
<h3><span class="section-number">6.2.3. </span>Splitting methods<a class="headerlink" href="#splitting-methods" title="Permalink to this headline">¶</a></h3>
<p>The proximal point methods require that the proximal operator for <span class="math notranslate nohighlight">\(R\)</span> can be evaluated efficiently. In many practical applications this is not the cases, however. Instead, we may have a regulariser of the form <span class="math notranslate nohighlight">\(R(Au)\)</span> for some linear operator <span class="math notranslate nohighlight">\(A\)</span>. Even when <span class="math notranslate nohighlight">\(R(\cdot)\)</span> admits an efficient proximal operator <span class="math notranslate nohighlight">\(R(A\cdot)\)</span> will, in general, not. In this section we discuss a class of methods that allow us to shift the operator <span class="math notranslate nohighlight">\(A\)</span> to the other part of the objective. As a model-problem we will consider solving</p>
<div class="math notranslate nohighlight">
\[\min_{u\in \mathbb{R}^n} D(u) + R(Au),\]</div>
<p>with <span class="math notranslate nohighlight">\(D\)</span> smooth and <span class="math notranslate nohighlight">\(\mu-\)</span> strongly convex, <span class="math notranslate nohighlight">\(R(\cdot)\)</span> convex and <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m\times n}\)</span> a linear map. The basic idea is to introduce an auxiliary variable <span class="math notranslate nohighlight">\(v\)</span> and re-formulate the variational problem as</p>
<div class="math notranslate nohighlight" id="equation-splitted">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-splitted" title="Permalink to this equation">¶</a></span>\[\min_{u\in \mathbb{R}^n,v\in\mathbb{R}^m} D(u) + R(v), \quad \text{s.t.} \quad Au = v.\]</div>
<p>The method of Lagrange multipliers defines the <em>Lagrangian</em></p>
<div class="math notranslate nohighlight">
\[\Lambda(u,v,\nu) = D(u) + R(v) + \nu^T(Au - v),\]</div>
<p>where <span class="math notranslate nohighlight">\(\nu \in \mathbb{R}^m\)</span> are called the Lagrange multipliers. The solution to <a class="reference internal" href="#equation-splitted">(6.6)</a> is a saddle point of <span class="math notranslate nohighlight">\(\Lambda\)</span> and we can thus be obtained by solving</p>
<div class="math notranslate nohighlight" id="equation-saddle-point">
<span class="eqno">(6.7)<a class="headerlink" href="#equation-saddle-point" title="Permalink to this equation">¶</a></span>\[\min_{u,v} \max_{\nu} \Lambda(u,v,\nu).$$\]</div>
<p>The equivalence between <a class="reference internal" href="#equation-splitted">(6.6)</a> and <a class="reference internal" href="#equation-saddle-point">(6.7)</a> is established in the following theorem</p>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem:</p>
<p>Let ..</p>
</div>
<hr class="docutils" />
<div class="admonition-definition-dual-problem admonition">
<p class="admonition-title">Definition: <em>Dual problem</em></p>
<p>Re-organising terms we get the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Duality_(optimization)"><em>dual problem</em></a></p>
<div class="math notranslate nohighlight">
\[\max_{\nu} \min_{u} D(u) + \nu^TAu + \min_v R(v) - \nu^Tv.\]</div>
</div>
<hr class="docutils" />
<p>We can now proceed to solve <a class="reference internal" href="#equation-saddle-point">(6.7)</a> in a number of ways. We will discuss two routes.</p>
<div class="admonition-alternating-direction-of-multipliers-admm admonition">
<p class="admonition-title"><em>Alternating Direction of Multipliers (ADMM)</em></p>
<p>We augment the Lagrangian by adding a quadratic term:</p>
<div class="math notranslate nohighlight">
\[\Lambda_{\rho}(u,v,\nu) = D(u) + R(v) + \nu^T(Au - v) + \rho \|Au - v\|_2^2.\]</div>
<p>We then find the solution by updating the variables in an alternating fashion</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \prox_{\alpha D}\left(u_k - (\alpha/\beta)A^T\left(Au_k - v_k + \nu_k\right)\right)\]</div>
<div class="math notranslate nohighlight">
\[v_{k+1} = \prox_{\beta R}\left(Au_k + \nu_k\right)\]</div>
<div class="math notranslate nohighlight">
\[\nu_{k+1} = \nu_k + Au_k - v_k\]</div>
</div>
<div class="admonition-dual-based-proximal-gradient admonition">
<p class="admonition-title"><em>Dual-based proximal gradient</em></p>
<p>Here, we recognise the <a class="reference external" href="https://en.wikipedia.org/wiki/Convex_conjugate"><em>convex conjugates</em></a> of <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(R\)</span>. With this, we re-write the problem as</p>
<div class="math notranslate nohighlight">
\[\min_{\nu} D^*(A^T\nu) + R^*(\nu).\]</div>
<p>Thus, we have moved the linear map to the other side. We can now apply the proximal gradient method provided that:</p>
<ul class="simple">
<li><p>We have a closed-form expression for the convex conjugates of <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(R\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(R^*\)</span> has a proximal operator that is easily evaluated.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="references">
<h2><span class="section-number">6.3. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-numerical_optimisation-0"><dl class="citation">
<dt class="bibtex label" id="beck2017"><span class="brackets">1</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Amir Beck. <em>First-Order Methods in Optimization</em>. Society for Industrial and Applied Mathematics, 2017.</p>
</dd>
<dt class="bibtex label" id="nocedal2006numerical"><span class="brackets">2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>,<a href="#id4">4</a>)</span></dt>
<dd><p>Jorge Nocedal and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.</p>
</dd>
</dl>
</p>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">6.4. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="steepest-descent-for-strongly-convex-functionals">
<h3><span class="section-number">6.4.1. </span>Steepest descent for strongly convex functionals<a class="headerlink" href="#steepest-descent-for-strongly-convex-functionals" title="Permalink to this headline">¶</a></h3>
<p>Consider the following fixed point iteration for minimizing a given function <span class="math notranslate nohighlight">\(J : \mathbb{R}^n \rightarrow \mathbb{R}\)</span></p>
<div class="math notranslate nohighlight">
\[
u^{(k+1)} = u^{(k)} - \alpha J'(u^{(k)}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(J\)</span> is twice continuously differentiable and strictly convex:</p>
<div class="math notranslate nohighlight">
\[
\mu I \preceq J''(u) \preceq L I,
\]</div>
<p>with <span class="math notranslate nohighlight">\(0 &lt; \mu &lt; L &lt; \infty\)</span>.</p>
<ul class="simple">
<li><p>Show that the fixed point iteration converges linearly for <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 2/L\)</span>.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Rate_of_convergence#Basic_definition">Linear convergence</a> implies that <span class="math notranslate nohighlight">\(\exists 0 &lt; \rho &lt; 1\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\|u^{(k+1)} - u^*\| \leq \rho \|u^{(k)} - u*\|,\]</div>
<p>where <span class="math notranslate nohighlight">\(u^*\)</span>. To show this we start from the iteration and substract the fixed-point and use that <span class="math notranslate nohighlight">\(\nabla f(u^*) = 0\)</span> to get</p>
<div class="math notranslate nohighlight">
\[(u^{(k+1)} - u^*) = (u^{(k)} - u^*) - \alpha (\nabla f(u^{(k)}) - \nabla f (u^*)).\]</div>
<p>Next use Taylor to express</p>
<div class="math notranslate nohighlight">
\[\nabla f(u^{(k)}) - \nabla f (u^*) = \nabla^2 f(\eta^{(k)}) (u^{(k)} - u^*),\]</div>
<p>with <span class="math notranslate nohighlight">\(\eta^{(k)} = t u^{(k)} + (1-t)u^*\)</span> for some <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>. We then get</p>
<div class="math notranslate nohighlight">
\[\|u^{(k+1)} - u^*\|_2 \leq \|I - \alpha \nabla^2 f(\eta^{(k)})\|_2 \|u^{(k)} - u^*\|_2.\]</div>
<p>For linear convergence we need <span class="math notranslate nohighlight">\(\|I - \alpha \nabla^2 f(\eta^{(k)})\|_2 &lt; 1\)</span>. We use that <span class="math notranslate nohighlight">\(\|A\|_2 = \sigma_{\max}(A)\)</span>. (cf. <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_norm#Special_cases">Matrix norms</a>)
Since the eigenvalues of <span class="math notranslate nohighlight">\(\nabla^2 f\)</span> are bounded by <span class="math notranslate nohighlight">\(L\)</span> we need <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 2/L\)</span> to ensure this.</p>
</div>
<ul class="simple">
<li><p>Determine the value of <span class="math notranslate nohighlight">\(\alpha\)</span> for which the iteration converges fastest.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<p>The smaller the bound on the constant <span class="math notranslate nohighlight">\(\rho\)</span>, the faster the convergence. We have</p>
<div class="math notranslate nohighlight">
\[\|I - \alpha \nabla^2 f(\eta^{(k)})\|_2 = \ \max (|1 - \alpha \mu|, |1 - \alpha L|).\]</div>
<p>We obtain the smalles possible value by making both terms equal, for which we need</p>
<div class="math notranslate nohighlight">
\[(1 - \alpha \mu) = -(1 - \alpha L),\]</div>
<p>this gives us an optimal value of <span class="math notranslate nohighlight">\(\alpha = 2/(\mu + L)\)</span>.</p>
</div>
</div>
<div class="section" id="rosenbrock">
<h3><span class="section-number">6.4.2. </span>Rosenbrock<a class="headerlink" href="#rosenbrock" title="Permalink to this headline">¶</a></h3>
<p>We are going to test various optimization methods on the Rosenbrock function</p>
<div class="math notranslate nohighlight">
\[
f(x,y) = (a - x)^2 + b(y - x^2)^2,
\]</div>
<p>with <span class="math notranslate nohighlight">\(a = 1\)</span> and <span class="math notranslate nohighlight">\(b = 100\)</span>. The function has a global minimum at <span class="math notranslate nohighlight">\((a, a^2)\)</span>.</p>
<ul class="simple">
<li><p>Write a function to compute the Rosenbrock function, its gradient and the Hessian for given input <span class="math notranslate nohighlight">\((x,y)\)</span>. Visualize the function on <span class="math notranslate nohighlight">\([-3,3]^2\)</span> and indicate the neighborhood around the minimum where <span class="math notranslate nohighlight">\(f\)</span> is convex.</p></li>
<li><p>Implement the method from exercise 1 and test convergence from various initial points. Does the method always convergce? How small do you need to pick <span class="math notranslate nohighlight">\(\alpha\)</span>? How fast?</p></li>
<li><p>Implement a linesearch strategy to ensure that <span class="math notranslate nohighlight">\(\alpha_k\)</span> satisfies the Wolfe conditions, does <span class="math notranslate nohighlight">\(\alpha\)</span> vary a lot?</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>In de code below, we show a graph of the function and determine the region of convexity by computing the eigenvalues of the Hessian (should be positive)</p></li>
<li><p>We observe linear convergence for small enough <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
<li><p>Using a linesearch we obtain faster convergence by allowing larger steps in the beginning.</p></li>
</ul>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">line_search</span>

<span class="c1"># rosenbrock function</span>
<span class="k">def</span> <span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x1</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)])</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">12</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">x1</span><span class="o">*</span><span class="n">b</span><span class="p">],[</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">H</span>

<span class="c1"># steepest descent</span>
<span class="k">def</span> <span class="nf">steep</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">niter</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">fk</span><span class="p">,</span><span class="n">gk</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gk</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># steepest descent with linesearch</span>
<span class="k">def</span> <span class="nf">steep_wolfe</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">alpha0</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">niter</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">fk</span><span class="p">,</span><span class="n">gk</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">pk</span> <span class="o">=</span> <span class="o">-</span><span class="n">alpha0</span><span class="o">*</span><span class="n">gk</span> <span class="c1">#reference stepsize</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">line_search</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">pk</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">alpha</span><span class="p">:</span> <span class="c1"># check if linesearch was successfull</span>
            <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">pk</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># if not, use regular step</span>
            <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">pk</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot of the Rosenbrock function</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">fs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">],</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">((</span><span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,</span><span class="n">fs</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x101b296a90&gt;]
</pre></div>
</div>
<img alt="_images/numerical_optimisation_7_1.png" src="_images/numerical_optimisation_7_1.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># determine region of convexity by computing eigenvalues of the Hessian</span>
<span class="n">e1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">e2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">Hs</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">((</span><span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
        <span class="n">e1</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">],</span><span class="n">e2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">Hs</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,(</span><span class="n">e1</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">e2</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">),</span><span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x101b881090&gt;]
</pre></div>
</div>
<img alt="_images/numerical_optimisation_8_1.png" src="_images/numerical_optimisation_8_1.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run steepest descent</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">12122</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.99</span><span class="o">/</span><span class="n">L</span>
<span class="n">maxiter</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">steep</span><span class="p">(</span><span class="n">rosenbrock</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span><span class="n">alpha</span><span class="p">,</span><span class="n">maxiter</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxiter</span><span class="p">,</span><span class="n">maxiter</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,</span><span class="n">fs</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xs</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">k</span><span class="p">,(</span><span class="o">.</span><span class="mi">99993</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;error&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_9_0.png" src="_images/numerical_optimisation_9_0.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run steepest descent with linesearch</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">12122</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.99</span><span class="o">/</span><span class="n">L</span>
<span class="n">maxiter</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">steep_wolfe</span><span class="p">(</span><span class="n">rosenbrock</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span><span class="mf">1.99</span><span class="o">/</span><span class="n">L</span><span class="p">,</span><span class="mi">50000</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxiter</span><span class="p">,</span><span class="n">maxiter</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,</span><span class="n">fs</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xs</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">k</span><span class="p">,(</span><span class="o">.</span><span class="mi">99993</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="stderr docutils container">
<pre class="stderr literal-block">/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)
/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)
</pre>
</div>
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x101c40e750&gt;,
 &lt;matplotlib.lines.Line2D at 0x101ca67710&gt;]
</pre></div>
</div>
<img alt="_images/numerical_optimisation_10_2.png" src="_images/numerical_optimisation_10_2.png" />
</div>
</div>
</div>
<div class="section" id="convex-conjugates">
<h3><span class="section-number">6.4.3. </span>Convex conjugates<a class="headerlink" href="#convex-conjugates" title="Permalink to this headline">¶</a></h3>
<p>Compute the convex conjugates of the following primal functionals <span class="math notranslate nohighlight">\(J : X \rightarrow \mathbb{R} \cup
\{ \infty \}\)</span>, i.e. the dual functionals <span class="math notranslate nohighlight">\(J^*: X^*\rightarrow \mathbb{R} \cup \{ \infty \}\)</span> of,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J(u) = \left\| u \right\|_{L^2(\Omega)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J(u) = \frac{\alpha}{2} \left\| u \right\|_{L^2(\Omega)}^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J(u) = \left\| u \right\|_{L^1(\Omega)}\)</span>.</p></li>
</ul>
<p><strong>Hint</strong>: Remark, that in the dual formulation, due to the supremum, there holds equality in the Hölder inequality.</p>
</div>
<div class="section" id="subdifferentials">
<h3><span class="section-number">6.4.4. </span>Subdifferentials<a class="headerlink" href="#subdifferentials" title="Permalink to this headline">¶</a></h3>
<p>Compute the subdifferentials <span class="math notranslate nohighlight">\(\partial f(x)\)</span> of the following functions:</p>
<ul class="simple">
<li><p>The Euclidean norm <span class="math notranslate nohighlight">\(f:\mathbb{R}^n \rightarrow \mathbb{R}, x \, \mapsto
  \left\|x\right\|_{\ell^2}\)</span></p></li>
<li><p>The characteristic function of the positive quadrant,
<span class="math notranslate nohighlight">\(\,f = \chi_K, \, \text{ with } \, K:=\left\{ x \in \mathbb{R}^n \,:\, x_j \geq 0, \text{ for all } \, 1 \leq j \leq n \right\}\)</span>.</p></li>
</ul>
<p>https://en.wikipedia.org/wiki/Characteristic_function_(convex_analysis)</p>
</div>
<div class="section" id="soft-tresholding">
<h3><span class="section-number">6.4.5. </span>Soft tresholding<a class="headerlink" href="#soft-tresholding" title="Permalink to this headline">¶</a></h3>
<p>In efficient splitting methods, e.g. in Split Bregman, see next exercise below, subproblems often can be reduced to proximal steps, like soft shrinkage.</p>
<ul class="simple">
<li><p>Hence, show in 1D <span class="math notranslate nohighlight">\((\Omega \subset \mathbb{R})\)</span>, that a solution <span class="math notranslate nohighlight">\(z^* : \Omega \rightarrow \mathbb{R}\)</span> of the functional</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
	\min_z \frac{1}{2}\left\| z-f \right\|_{L^2(\Omega)}^2 + \alpha \left\| z \right\|_{L^1(\Omega)}
\]</div>
<p>is explicitly given by the application of the soft shrinkage operator <span class="math notranslate nohighlight">\(S_\alpha(f)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split} z^* = S_\alpha(f) := \left\{
\begin{align*}
&amp;f - \alpha , &amp;\text{if}\: f &gt; \alpha \\
&amp;0,           &amp;\text{if}\: -\alpha \leq f \leq \alpha\\
&amp;f + \alpha , &amp;\text{if}\: f &lt; -\alpha
\end{align*}
\right\}
\end{split}\]</div>
<ul class="simple">
<li><p>What would happen with this formula if we would go from convex regularization to nonconvex regularization, i.e. <span class="math notranslate nohighlight">\(L^p(\Omega)\)</span> with <span class="math notranslate nohighlight">\(0 &lt; p &lt; 1\)</span> instead of <span class="math notranslate nohighlight">\(L^1(\Omega)\)</span> in the regularization? (This is a difficult question. Search for hard shrinkage to get an idea.)</p></li>
</ul>
</div>
<div class="section" id="a-dual-method-for-tv-denoising">
<h3><span class="section-number">6.4.6. </span>A dual method for TV denoising<a class="headerlink" href="#a-dual-method-for-tv-denoising" title="Permalink to this headline">¶</a></h3>
<p><strong>Proof</strong> that the Rudin-Osher-Fatemi (ROF) minimization for denoising with <span class="math notranslate nohighlight">\(L^2\)</span> data fidelity and TV regularisation:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2}\|u - f\|_{L^2}^2 + \alpha \text{TV}(u)\]</div>
<p>is equivalent (in the sense of the same local minima) to the following dual minimization problem</p>
<div class="math notranslate nohighlight">
\[J(g) := \frac{1}{2} \int_\Omega \left(\alpha \nabla \cdot  g - f\right)^2 \rightarrow \min_g\]</div>
<p>under the constraint <span class="math notranslate nohighlight">\(\lVert g \rVert_{L^\infty} \leq 1\)</span>. This is a constrained quadratic optimisation problem. The constraint should be interpreted as <span class="math notranslate nohighlight">\(|g(x)|_{l^2}^2 \leq 1,\, \forall x \in \Omega\)</span>.</p>
<p><strong>Write code</strong> which performs the explicit discretisation</p>
<div class="math notranslate nohighlight">
\[g_{k+\frac{1}{2}} = g_k + \beta \: \nabla \left(\alpha \nabla \cdot g_k -f\right)\]</div>
<div class="math notranslate nohighlight">
\[g_{k+1} = \Pi(g_{k+\frac{1}{2}})\qquad\qquad\quad\]</div>
<p>where <span class="math notranslate nohighlight">\(\Pi(g) := \frac{g}{\lVert g \rVert}_{L^\infty}\)</span> denotes a projection onto the unit circle. You can easily discretize the divergence and gradient as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">h</span>
<span class="n">div</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<p><strong>Test</strong> your implementation on a 1D step function with additional random Gaussian noise.</p>
<p><strong>Compare</strong> the solutions for different values of <span class="math notranslate nohighlight">\(\alpha\)</span> and choose the step size <span class="math notranslate nohighlight">\(\beta\)</span> adequately.</p>
<p>Hint: Use the primal optimality condition of the ROF model (with exact, dual definition of TV), to be able to visualise the primal solution <span class="math notranslate nohighlight">\(u^*\)</span> out of the corresponding, computed dual solution <span class="math notranslate nohighlight">\(g^*\)</span>.</p>
<p>Example code is shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># grid \Omega = [0,1]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># parameters</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1e-1</span>

<span class="c1"># make data</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">u</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_13_0.png" src="_images/numerical_optimisation_13_0.png" />
</div>
</div>
</div>
<div class="section" id="a-prima-dual-method-for-tv-denoising">
<h3><span class="section-number">6.4.7. </span>A Prima-dual method for TV denoising<a class="headerlink" href="#a-prima-dual-method-for-tv-denoising" title="Permalink to this headline">¶</a></h3>
<p>In the lecture we have introduced with Split-Bregman, or equivalently Alternating direction method of multipliers (ADMM), a splitting method, with which we can solve the ROF model</p>
<div class="math notranslate nohighlight">
\[ \min_u \frac{1}{2} \left\| u-f \right\|_{L^2(\Omega)} + \alpha \left\| \nabla u \right\|_{L^1(\Omega)}\]</div>
<p>efficiently in an alternating primal-dual fashion.</p>
<ul class="simple">
<li><p>Derive the splitting method in 1D <span class="math notranslate nohighlight">\((\Omega \subset \mathbb{R})\)</span> analogous to the lecture for the ROF model, implement it in Python and test it for different step sizes and regularisation parameters for a step function with additive Gaussian noise.</p></li>
<li><p>How do the subproblems of the splitting algorithm change, if we make the transition from denoising to reconstruction with an operator <span class="math notranslate nohighlight">\(K:\Omega \rightarrow \Omega\)</span>, without introducing additional constraints? Which property would the operator <span class="math notranslate nohighlight">\(K\)</span> need, such that the whole method could still be realised efficiently via FFT and DCT inside?</p></li>
</ul>
</div>
</div>
<div class="section" id="assignments">
<h2><span class="section-number">6.5. </span>Assignments<a class="headerlink" href="#assignments" title="Permalink to this headline">¶</a></h2>
<div class="section" id="spline-regularisation">
<h3><span class="section-number">6.5.1. </span>Spline regularisation<a class="headerlink" href="#spline-regularisation" title="Permalink to this headline">¶</a></h3>
<p>The aim is to solve the following variational problem</p>
<div class="math notranslate nohighlight">
\[\min_u \frac{1}{2} \|Ku - f^{\delta}\|_2^2 + \alpha \|Lu\|_1,\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is a given forward operator (matrix) and <span class="math notranslate nohighlight">\(L\)</span> is a discretisation of the second derivative operator.</p>
<ol class="simple">
<li><p>Design and implement a method for solving this variational problem; you can be creative here – multiple answers are possible</p></li>
<li><p>Compare your method with the basic subgradient-descent method implemented below</p></li>
<li><p>(bonus) Find a suitable value for <span class="math notranslate nohighlight">\(\alpha\)</span> using the discrepancy principle</p></li>
</ol>
<p>Some code to get you started is shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># forward operator</span>
<span class="k">def</span> <span class="nf">getK</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">h</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">h</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">xx</span> <span class="o">-</span> <span class="n">yy</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">K</span><span class="p">,</span><span class="n">x</span>

<span class="c1"># define regularization operator</span>
<span class="k">def</span> <span class="nf">getL</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">;</span>
    <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">h</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">L</span>

<span class="c1"># define grid and operators</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">K</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">getK</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">getL</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># true solution and corresponding data</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">0.5</span><span class="o">-</span><span class="n">x</span><span class="p">),</span><span class="mf">0.3</span> <span class="o">+</span> <span class="mi">0</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">K</span><span class="nd">@u</span>

<span class="c1"># noisy data</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">f</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">noise</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_15_0.png" src="_images/numerical_optimisation_15_0.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example implementation of subgradient-descent</span>
<span class="k">def</span> <span class="nf">subgradient</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">f_delta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">niter</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="c1"># keep track of function value</span>
        <span class="n">objective</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">K</span><span class="nd">@u</span> <span class="o">-</span> <span class="n">f_delta</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="nd">@u</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># compute (sub) gradient</span>
        <span class="n">gr</span> <span class="o">=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">T</span><span class="o">@</span><span class="p">(</span><span class="n">K</span><span class="nd">@u</span> <span class="o">-</span> <span class="n">f_delta</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="nd">@np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">L</span><span class="nd">@u</span><span class="p">))</span>
        <span class="c1"># update with stepsize t</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">-</span> <span class="n">t</span><span class="o">*</span><span class="n">gr</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">objective</span>

<span class="c1"># get data</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="n">K</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">getK</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">getL</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">0.5</span><span class="o">-</span><span class="n">x</span><span class="p">),</span><span class="mf">0.3</span> <span class="o">+</span> <span class="mi">0</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">K</span><span class="nd">@u</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">noise</span>

<span class="c1"># parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">niter</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">t</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># run subgradient descent</span>
<span class="n">uhat</span><span class="p">,</span> <span class="n">objective</span> <span class="o">=</span> <span class="n">subgradient</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">f_delta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">niter</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">objective</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective value&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ground truth&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">uhat</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;reconstruction&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;u(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;u(x)&#39;)
</pre></div>
</div>
<img alt="_images/numerical_optimisation_16_1.png" src="_images/numerical_optimisation_16_1.png" />
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="variational_formulations.html" title="previous page"><span class="section-number">5. </span>Variational formulations for inverse problems</a>
    <a class='right-next' id="next-link" href="image_processing.html" title="next page"><span class="section-number">1. </span>Image processing</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Tristan van Leeuwen and Christoph Brune (CC BY-NC 4.0)<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>